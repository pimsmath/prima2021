---
title: Neural Network Approximation of Lipschitz Functions in High Dimensions
sessions:
- Mathematics of Information
speakers:
- name: Santhosh Karnik
  affiliation: Michigan State University
locations:
- Junior Ballroom C
tags:
- ''
outputs:
- HTML
- Calendar
categories:
- special-session
date: '2022-12-06T10:00:00-08:00'
publishDate: '2022-10-01T10:00:00-08:00'
end: '2022-12-06T11:00:00-08:00'
featured: 'false'
draft: 'false'
abstract: The remarkable successes of neural networks in a huge variety of inverse
  problems have fueled their adoption in disciplines ranging from medical imaging
  to seismic analysis over the past decade. However, the high dimensionality of such
  inverse problems has simultaneously left current theory, which predicts that networks
  should scale exponentially in the dimension of the problem, unable to explain why
  the seemingly small networks used in these settings work as well as they do in practice.
  To reduce this gap between theory and practice, we provide a general method for
  bounding the complexity required for a neural network to approximate a Lipschitz
  function on a high-dimensional set with a low-complexity structure. The approach
  is based on the fact that many sets of interest in high dimensions have low-distortion
  linear embeddings into lower dimensional spaces. We can exploit this fact to show
  that the size of a neural network needed to approximate a Lipschitz function on
  a low-complexity set in a high dimensional space grows exponentially with the dimension
  of its low-distortion embedding, not the dimension of the space it lies in.
---
