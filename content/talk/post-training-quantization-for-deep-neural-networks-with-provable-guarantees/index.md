---
title: 'Post-training Quantization for Deep Neural Networks with Provable
Guarantees'
  and power consumption of deep neural networks'
sessions:
- Mathematics of Information
speakers:
- name: Jinjie Zhang
  affiliation: University of California San Diego
locations:
- Junior Ballroom C
tags:
- ''
outputs:
- HTML
- Calendar
categories:
- special-session
date: '2022-12-06T10:00:00-08:00'
end: '2022-12-06T11:00:00-08:00'
featured: 'false'
draft: 'false'
abstract: |
  Quantization is one of the compression techniques to reduce computation cost,
  memory, and power consumption of deep neural networks (DNNs). In this talk, we
  will focus on a post-training quantization algorithm, GPFQ, that is based on a
  deterministic greedy path-following mechanism, and its stochastic variant
  SGPFQ. In both cases, we rigorously analyze the associated error bounds for
  quantization and show that for quantizing a single-layer network, the relative
  square error essentially decays linearly in the number of weights -- i.e.,
  level of over-parametrization. To empirically evaluate the method, we quantize
  several common DNN architectures with few bits per weight, and test them on
  ImageNet, showing only minor loss of accuracy compared to unquantized models.
---
