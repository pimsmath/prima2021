---
title: Function approximation with one-bit Bernstein and one-bit neural networks
sessions:
- Mathematics of Information
speakers:
- name: Weilin Li
  affiliation: CUNY City College
locations:
- Junior Ballroom C
tags:
- ''
outputs:
- HTML
- Calendar
categories:
- special-session
date: 2022-12-08T17:00:00-0800
publishDate: '2022-10-01T10:00:00-08:00'
end: 2022-12-08T17:25:00-0800
featured: 'false'
draft: 'true'
abstract: The celebrated universal approximation theorems for neural networks typically
  state that every sufficiently nice function can be arbitrarily well approximated
  by a neural network with carefully chosen parameters. Motivated by applications
  where compression is necessary, we ask whether it is possible to represent any reasonable
  function with a neural network whose parameters are restricted to a small set of
  values, with the extreme case being one-bit {+1,-1} neural networks? We answer this
  question in the affirmative for both quadratic and ReLU networks. Our main contributions
  include a novel approximation result for {+1,-1} sums of multivariate Bernstein
  polynomials and implementation of this approximation strategy via one-bit neural
  networks with either the quadratic or ReLU activation functions. Joint work with
  Sinan Gunturk.
---
